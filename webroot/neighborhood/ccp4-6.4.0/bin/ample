#!/usr/bin/env python

"""
Version 0.1
no speed buffs, no extra functionality, no optimisation
Basic Pipeline
for multi core desktops, not clusters

Issues:
1) if this script is killed, the rosetta will continue
2) Theseus can hang, this is killed after a timeout
3) is clustered by all atom RMSD to make fine clusters (option to cluster by CA only i included)
4) ASU content is the number of search models placed by MrBUMP. -- need to set this manually


================================

so...
Only data passed between different stages are the pdb files, so each stage could just start from a directory of files

"""

import os
import sys

# Test for environment variables
if not "CCP4" in sorted(os.environ.keys()):
    raise RuntimeError('CCP4 not found')

# Add the ample python folder to the PYTHONPATH
sys.path.append(os.path.join(os.environ["CCP4"], "share", "ample", "python"))

# python imports
import argparse
import clusterize
import cPickle
import glob
import logging
import shutil
import time

# Our imports
import ample_options
import ample_util
import cif_parser
import ensemble
import fasta_parser
import mrbump_ensemble
import nmr
import rosetta_model
import mrbump_results
import pdb_edit
import truncateedit_MAX


#-------------------------------------------
# get command line options
#-------------------------------------------

parser = argparse.ArgumentParser(description='Structure solution by abinitio modelling', prefix_chars="-")

parser.add_argument('-alignment_file', metavar='alignment_file', type=str, nargs=1,
                   help='alignment between homolog and target fastas')

parser.add_argument('-all_atom', metavar='all_atom', type=str, nargs=1,
                   help='all_atom')

parser.add_argument('-arpwarp_cycles', metavar='arpwarp_cycles', type=int, nargs=1,
                   help='number of cycles ')

parser.add_argument('-ASU', metavar='no in ASU', type=int, nargs=1,
                   help='no in ASU')

parser.add_argument('-blast_dir', metavar='blast_dir', type=str, nargs=1,
                   help='Directory where ncbi blast is installed (binaries in bin)')

parser.add_argument('-buccaneer_cycles', metavar='buccaneer_cycles', type=int, nargs=1,
                   help='number of cycles ')

parser.add_argument('-CC', metavar='rad of gyration', type=str, nargs=1,
                   help='radius of gyration reweight ')

parser.add_argument('-ccp4_jobid', metavar='ccp4_jobid', type=int, nargs=1,
                   help='CCP4 job id - only needed when running from the CCP4 GUI')

parser.add_argument('-submit_cluster', metavar='submit_cluster', type=str, nargs=1,
                   help='will submit jobs to a cluster')

parser.add_argument('-submit_qtype', metavar='submit_qtype', type=str, nargs=1,
                   help='cluster submission queue type:SGE, LSF...')

parser.add_argument('-debug', metavar='debug option', type=str, nargs=1,
                   help='debug option ')

parser.add_argument('-domain_all_chains_pdb', metavar='domain_all_chains_pdb', type=str, nargs=1,
                   help='fixed input to mr bump')

parser.add_argument('-domain_termini_distance', metavar='termini', type=str, nargs=1,
                   help='distance between termini for insert domains')

parser.add_argument('-early_terminate', metavar='early_terminate', type=str, nargs=1,
                     help='terminate early if a success')

parser.add_argument('-ensembler', metavar='ensembler', type=str, nargs=1,
                   help='ensembling')

parser.add_argument('-ensembles_dir', metavar='ensembles_dir', type=str, nargs=1,
                   help='path to directory containing existing ensembles')

parser.add_argument('-fasta', metavar='fasta_file', type=str, nargs=1,
                   help='protein fasta file. (required)')

parser.add_argument('-F', metavar='flag for F', type=str, nargs=1,
                   help='Flag for F')

parser.add_argument('-frags_3mers', metavar='frags_3mers', type=str, nargs=1,
                   help='path of imported 3mers')

parser.add_argument('-frags_9mers', metavar='frags_9mers', type=str, nargs=1,
                   help='path of imported 9mers')

parser.add_argument('-FREE', metavar='flag for FREE', type=str, nargs=1,
                   help='Flag for FREE')

parser.add_argument('-import_cluster', metavar='import_cluster', type=str, nargs=1,
                   help='import_cluster')

parser.add_argument('-improve_template', metavar='improve template', type=str, nargs=1,
                   help='give a template to imrove - NMR, homolog ')

parser.add_argument('-LGA', metavar='path_to_LGA dir', type=str, nargs=1,
                   help='pathway to LGA folder (not the exe) will use the \'lga\' executable')

parser.add_argument('-make_frags', metavar='bool to make fragments', type=str, nargs=1,
                   help='Bool, True to make non homologous framents, False to import fragments')

parser.add_argument('-make_models', metavar='Do the modelling', type=str, nargs=1,
                   help='run rosetta modeling, set to False to import pre-made models (required if making models locally default True)')

parser.add_argument('-maxcluster_exe', metavar='Maxcluster exe', type=str, nargs=1,
                   help='Maxcluster exe')

parser.add_argument('-max_ensemble_models', metavar='max_ensemble_models', type=str, nargs=1,
                   help='Maximum number of models permitted in an ensemble')

parser.add_argument('-missing_domain', metavar='missing_domain', type=str, nargs=1,
                   help='Modelling a missing domain - requires domain_all_chains_pdb argument')

parser.add_argument('-models_dir', metavar='folder of decoys', type=str, nargs=1,
                   help='folder of decoys')

parser.add_argument('-mr_keys', metavar='-mr_keys', type=str, nargs=1,
                   help='mrbump keywords ')

parser.add_argument('-mtz', metavar='MTZ in', type=str, nargs=1,
                   help='MTZ in')

parser.add_argument('-name', metavar='priotein name', type=str, nargs=1,
                   help='name of protien in the format ABCD ')

parser.add_argument('-nmodels', metavar='no models', type=int, nargs=1,
                   help='number of models to make (1000)')

parser.add_argument('-nr', metavar='nr', type=str, nargs=1,
                   help='Path to the nr non-redundant sequence database')

parser.add_argument('-old_shelx', metavar='old_shelx', type=str, nargs=1,
                   help='old_shelx')

parser.add_argument('-NMR_model_in', metavar='NMR_model_in', type=str, nargs=1,
                   help='use nmr input')

parser.add_argument('-NMR_process', metavar='NMR_process', type=str, nargs=1,
                   help='number of times to process the models')

parser.add_argument('-NMR_remodel_fasta', metavar='-NMR_remodel_fasta', type=str, nargs=1,
                   help='fasta_for_remodeling')

parser.add_argument('-NMR_Truncate_only', metavar='-NMR_Truncate_only', type=str, nargs=1,
                   help='do no remodelling only truncate the NMR')

parser.add_argument('-nproc', metavar='NoProcessors', type=int, nargs=1,
                   help='number of processers (default 1)')

parser.add_argument('-num_clusters', metavar='num clusters to sample', type=int, nargs=1,
                   help='number of clusters to sample')

parser.add_argument('-output_pdb', metavar='output_pdb', type=str, nargs=1,
                   help='name of the result pdb to output')

parser.add_argument('-percent', metavar='no clus to sample', type=str, nargs=1,
                   help='percent interval for truncation')

parser.add_argument('-phenix_exe', metavar='PHENIX.ensembler exe', type=str, nargs=1,
                   help='path to phenix executable')

parser.add_argument('-rosetta_path', metavar='rosetta_path', type=str, nargs=1,
                   help='path for Rosetta AbinitioRelax')

parser.add_argument('-ROSETTA_cluster', metavar='path to Rosettas cluster', type=str, nargs=1,
                   help='location of rosetta cluster')

parser.add_argument('-rosetta_db', metavar='ROSETTA_database', type=str, nargs=1,
                   help='path for Rosetta database')

parser.add_argument('-rosetta_dir', metavar='Rosetta_dir', type=str, nargs=1,
                   help='the Rosetta install directory')

parser.add_argument('-rosetta_fragments_exe', metavar='path to make_fragments.pl', type=str, nargs=1,
                   help='location of make_fragments.pl')

parser.add_argument('-rosetta_version', metavar='rosetta_version', type=float, nargs=1,
                   help='the version number of Rosetta')

parser.add_argument('-run_dir', metavar='run_directory', type=str, nargs=1,
                   help='directory to put files (default current dir)')

parser.add_argument('-scwrl_exe', metavar='path to scwrl', type=str, nargs=1,
                   help='pathway to SCWRL exe')

parser.add_argument('-sf_cif', metavar='sf_cif', type=str, nargs=1,
                   help='path to a structure factor CIF file')

parser.add_argument('-shelx_cycles', metavar='shelx_cycles', type=str, nargs=1,
                     help='number of shelx sycles')

parser.add_argument('-shelxe_exe', metavar='-shelxe_exe', type=str, nargs=1,
                   help='Set shelxe executable name (default "shelxe")')

parser.add_argument('-SIGF', metavar='flag for SIGF', type=str, nargs=1,
                   help='Flag for SIGF')

parser.add_argument('-spicker_exe', metavar='boolean', type=str, nargs=1,
                   help='path to spicker executable')

parser.add_argument('-split_mr', metavar='boolean', type=str, nargs=1,
                   help='whether to split MRBUMP MR jobs into separate jobs')

parser.add_argument('-theseus_exe', metavar='Theseus exe (required)', type=str, nargs=1,
                   help='path to theseus executable')

parser.add_argument('-top_model_only', metavar='top_model_only', type=str, nargs=1,
                   help='top_model_only')

parser.add_argument('-transmembrane', metavar='transmembrane', type=str, nargs=1,
                   help='Do modelling for transmembrane proteins')

parser.add_argument('-transmembrane_spanfile', metavar='transmembrane_spanfile', type=str, nargs=1,
                   help='span file for modelling transmembrane proteins')

parser.add_argument('-transmembrane_lipofile', metavar='transmembrane_lipofile', type=str, nargs=1,
                   help='lips4 file for modelling transmembrane proteins')

parser.add_argument('-use_homs', metavar='nohoms rosetta flag', type=str, nargs=1,
                   help='True =use nhomologs, False= dont use them ')

parser.add_argument('-use_arpwarp', metavar='-use_arpwarp', type=str, nargs=1,
                   help='True to use arpwarp ')

parser.add_argument('-use_buccaneer', metavar='use_buccaneer', type=str, nargs=1,
                   help='True to use Buccaneer')

parser.add_argument('-use_scwrl', metavar='USE_SCWRL', type=str, nargs=1,
                   help='if using scwrl true if not false')

parser.add_argument('-use_shelxe', metavar='-use_shelxe', type=str, nargs=1,
                   help='True to use shelx ')

# Mutually exclusive options
group = parser.add_mutually_exclusive_group()
group.add_argument('-devel_mode', metavar='devel_mode', type=str, nargs=1,
                   help='preset options to run in development mode - takes longer')
group.add_argument('-quick_mode', metavar='quick_mode', type=str, nargs=1,
                   help='preset options to run quickly, but less thoroughly')

group = parser.add_mutually_exclusive_group()
group.add_argument('-molrep_only', metavar='molrep_only', type=str, nargs=1,
                   help='molrep_only')
group.add_argument('-phaser_only', metavar='phaser_only', type=str, nargs=1,
                   help='phaser_only')


# Create amopt object to hold options
amopt = ample_options.AmpleOptions( )

# MRkeys hack - get MRBUMP keywords direct as there can be multiple arguments
# to each one
MRkeys = []
#print sys.argv
keycount = 0
toRemove = [] # We remove all the mrkeywords
while keycount < len(sys.argv):
    #print sys.argv[keycount] ,  keycount
    if sys.argv[keycount] == "-mr_keys":
        toRemove.append(keycount)
        keycount += 1
        tmp = []
        while not sys.argv[keycount].startswith("-"):
            #MRkeys.append( sys.argv[keycount] )
            tmp.append( sys.argv[keycount] )
            toRemove.append( keycount )
            keycount += 1
            if keycount == len(sys.argv):
                break
        # Got last of list so add to MRkeys
        MRkeys.append( tmp )
        continue
    keycount += 1

# Need to decrement the index of the items to remove
# as we remove them
for count, i in enumerate(toRemove):
    del sys.argv[i-count]
## End MRKeys hack..

# convert args to dictionary
args = parser.parse_args()

# Now put them in the amopt object - this also sets/checks any defaults
amopt.populate( args )

# Now set MRKeys - might already have pre-set values so check if None or a list
if isinstance( amopt.d['mr_keys'],list ):
    amopt.d['mr_keys'] += MRkeys
else:
    amopt.d['mr_keys'] = MRkeys

# Make a work directory and go there - this way all output goes into this directory
if not os.path.exists(amopt.d['run_dir']):
    print 'You need to give a run directory'
    sys.exit()

print 'Making a Run Directory  -checking for previous runs\n'
amopt.d['work_dir'] = ample_util.make_workdir( amopt.d['run_dir'], ccp4_jobid=amopt.d['ccp4_jobid'] )
#amopt.d['work_dir'] = ample_util.make_workdir( amopt.d['run_dir'], rootname="ENSEMBLE_2_" )
os.chdir( amopt.d['work_dir'] )

# Set up logging
logger = ample_util.setup_logging()

# Path for pickling results
amopt.d['results_path'] = os.path.join( amopt.d['work_dir'], "resultsd.pkl" )

# cluster queueing
if amopt.d['submit_cluster'] and not amopt.d['submit_qtype']:
    msg = 'Must use -submit_qtype argument to specify queueing system (e.g. QSUB, LSF ) if submitting to a cluster.'
    logger.critical(msg)
    sys.exit(1)

#jmht - fix these
FIXED_INPUT = True
pdb_code = amopt.d['name']
amopt.d['pdb_code'] = amopt.d['name']
if not amopt.d['pdb_code'] or len(pdb_code) != 4:
    msg = 'Name is the wrong length, use 4 chars eg ABCD'
    logger.critical(msg)
    sys.exit(1)
else:
    amopt.d['pdb_code'] += '_'
    
# Check we can find the input fasta
if not ( os.path.exists(str(amopt.d['fasta']) ) or os.path.exists( str( amopt.d['NMR_remodel_fasta'] ) ) ):
    msg = 'You need to give the path for the fasta'
    logger.critical(msg)
    sys.exit(1)

# Reformat to what we need    
logger.debug('Parsing FASTA file')
outfasta = os.path.join( amopt.d['work_dir'], amopt.d['pdb_code'] + '_.fasta')
fp = fasta_parser.FastaParser()
fp.reformat_fasta( amopt.d['fasta'], outfasta )
amopt.d['fasta'] = outfasta
amopt.d['fasta_length'] = fp.length
logger.info( "Fasta is {0} amino acids long".format( amopt.d['fasta_length'] ) )

# Check we have a decent length
if amopt.d['fasta_length'] < 9:
    msg = "Fasta is of length {0}. Please check this is correct".format( amopt.d['fasta_length'] )
    logger.critical(msg)
    sys.exit(1)   

# Check we will be able to truncate at this level
if ( float( amopt.d['fasta_length'] ) / 100 ) * float( amopt.d['percent'] ) < 1:
    msg = "Cannot truncate a fasta sequence of length {0} with {1} percent intervals. Please select a larger interval.".format( amopt.d['fasta_length'], amopt.d['percent'] )
    logger.critical(msg)
    sys.exit(1)
    
# Check if importing ensembles
amopt.d['import_ensembles'] = False
if amopt.d['ensembles_dir']:
    msg=None
    if not os.path.isdir( amopt.d['ensembles_dir'] ):
        msg = "Trying to import ensembles from the directory:\n{0}\nBut it is not a directory!".format(amopt.d['ensembles_dir'])
    elif not len( glob.glob( os.path.join( amopt.d['ensembles_dir'], "*.pdb" ) ) ):
        msg = "Trying to import ensembles from the directory:\n{0}\nBut cannot find any pdb files!".format(amopt.d['ensembles_dir'])
    if msg:
        logger.critical(msg)
        sys.exit(1)

    amopt.d['import_ensembles'] = True
    logger.info("Found directory with ensemble files: {0}\n".format( amopt.d['ensembles_dir'] ) )
    amopt.d['make_frags'] = False
    amopt.d['make_models'] = False
    
# Check if importing models
amopt.d['import_models'] = False
if amopt.d['models_dir']:
    msg=None
    if not os.path.isdir( amopt.d['models_dir'] ):
        msg = "Trying to import models from the directory:\n{0}\nBut it is not a directory!".format(amopt.d['models_dir'])
    elif not len( glob.glob( os.path.join( amopt.d['models_dir'], "*.pdb" ) ) ):
        msg = "Trying to import models from the directory:\n{0}\nBut cannot find any pdb files!".format(amopt.d['models_dir'])
    if msg:
        logger.critical(msg)
        sys.exit(1)
    
    amopt.d['import_models'] = True
    amopt.d['make_frags'] = False
    amopt.d['make_models'] = False

if amopt.d['make_models']:
    # Create the models_dir
    amopt.d['models_dir'] = amopt.d['work_dir'] + os.sep + "models"
    os.mkdir( amopt.d['models_dir'] ) 
    # If the user has given both fragment files we check they are ok and unset make_frags
    if amopt.d['frags_3mers'] and amopt.d['frags_9mers']:
        if not os.path.isfile( amopt.d['frags_3mers'] ) or not os.path.isfile( amopt.d['frags_9mers'] ):
            msg = "frags_3mers and frag_9mers files given, but cannot locate them:\n{0}\n{1}\n".format( amopt.d['frags_3mers'], amopt.d['frags_9mers'] )   
            logger.critical(msg)
            sys.exit(1)
        amopt.d['make_frags'] = False
    
    if amopt.d['make_frags'] and (  amopt.d['frags_3mers'] or  amopt.d['frags_9mers'] ):
        msg = "make_frags set to true, but you have given the path to the frags_3mers or frags_9mers"
        logger.critical(msg)
        sys.exit(1)
    
    if not amopt.d['make_frags'] and not (  amopt.d['frags_3mers'] and  amopt.d['frags_9mers'] ):
        msg = "make_frags set to false, but you have not given the paths to the frags_3mers or frags_9mers"
        logger.critical(msg)
        sys.exit(1)   

# Check models and ensembles
if amopt.d['import_ensembles'] and amopt.d['import_models']:
        msg = "Cannot import both models and ensembles!"
        logger.critical(msg)
        sys.exit(1)

# NMR Checks
NMR_PROTOCOL = False
if amopt.d['NMR_model_in']:
    
    if not os.path.isfile( amopt.d['NMR_model_in'] ):
        msg = "NMR_model_in flag given, but cannot find file: {0}".format( amopt.d['NMR_model_in'] )
        logger.critical(msg)
        sys.exit(1)    
       
    NMR_PROTOCOL = True
    ROSETTA_CM = amopt.d['rosetta_dir'] + '/rosetta_source/bin/mr_protocols.default.linuxgccrelease'
    if not os.path.exists(ROSETTA_CM) :
        msg = 'Cannot find {0} check path names'.format( ROSETTA_CM )
        logger.critical( msg )
        sys.exit(1)

    if not os.path.isfile( str(amopt.d['NMR_remodel_fasta']) ):
        amopt.d['NMR_remodel_fasta'] =  amopt.d['fasta']

if not  NMR_PROTOCOL :
    if amopt.d['import_models'] and not os.path.exists(amopt.d['models_dir']):
        logger.warn('you have chosen to import models, but path does not exist, looking for ensembles : ' + amopt.d['models_dir'])
        if not os.path.exists(amopt.d['ensembles_dir']):
            logger.warn('you have chosen to import ensembles, but path does not exist : ' + amopt.d['ensembles_dir'])
            if amopt.d['import_cluster']:
                logger.info('you are importing a cluster of models from :' + amopt.d['import_cluster'])
                if not os.path.exists(amopt.d['import_cluster']):
                    logger.critical('you have chosen to import a cluster, the clusterpath does not exist')
                    sys.exit(1)

# Missing domains
if amopt.d['missing_domain']:
    logger.info('Processing missing domain\n')
    if not os.path.exists( amopt.d['domain_all_chains_pdb'] ):
        msg = 'Cannot find file domain_all_chains_pdb: {0}'.format( amopt.d['domain_all_chains_pdb'] )
        logger.critical( msg )
        sys.exit(1)

# MR programs
if amopt.d['molrep_only'] and amopt.d['phaser_only']:
        logger.critical('you say you want molrep only AND phaser only, choose one or both')
        sys.exit(1)
if amopt.d['molrep_only']:
    amopt.d['mrbump_programs'] = [ 'molrep' ]
elif amopt.d['phaser_only']:
    amopt.d['mrbump_programs'] = [ 'phaser' ]
else:
    amopt.d['mrbump_programs'] = ['molrep', 'phaser']

#
#Check we can find all the required programs
#
if amopt.d['ensembler']:
    logger.info('You are using Phenix ensembler')
    amopt.d['phenix_exe'] = ample_util.check_for_exe('phenix', amopt.d['phenix_exe'])
else:
    amopt.d['theseus_exe'] = ample_util.check_for_exe('theseus', amopt.d['theseus_exe'])

amopt.d['maxcluster_exe'] = ample_util.check_for_exe('maxcluster', amopt.d['maxcluster_exe'])
amopt.d['spicker_exe'] = ample_util.check_for_exe('spicker', amopt.d['spicker_exe'])

if amopt.d['use_scwrl']:
    amopt.d['scwrl_exe'] = ample_util.check_for_exe('Scwrl4', amopt.d['scwrl_exe'] )
if amopt.d['use_shelxe']:
    amopt.d['shelxe_exe'] = ample_util.check_for_exe('shelxe', amopt.d['shelxe_exe'])

# Create the rosetta modeller - this runs all the checks required
if amopt.d['make_models'] or amopt.d['make_frags']:  # only need Rosetta if making models
    logger.info('Making models or fragments so checking ROSETTA options')
    rosetta_modeller = rosetta_model.RosettaModel()
    rosetta_modeller.set_from_dict( amopt.d )

# Check if we need to get any of the flags from the MTZ file
if amopt.d['sf_cif'] and amopt.d['mtz']:
    logger.critical("Both sf_cif and mtz flags supplied - only use one")
    sys.exit(1)

# We've been given a sf_cif so convert
if amopt.d['sf_cif']:
    if not os.path.isfile( amopt.d['sf_cif'] ):
        logger.critical("Cannot find sf_cif file: {0}".format( amopt.d['sf_cif'] ) )
        sys.exit(1)
    else:
        cp = cif_parser.CifParser()
        amopt.d['mtz'] = cp.sfcif2mtz( amopt.d['sf_cif'] )
    
if not os.path.isfile( amopt.d['mtz'] ):
    logger.critical("Cannot find MTZ file: {0}".format( amopt.d['mtz'] ) )
    sys.exit(1)

if not amopt.d['F'] or not amopt.d['SIGF'] or not amopt.d['FREE']:
    
    logger.info("Determining MTZ flags using mtzdump from file: {0}".format( amopt.d['mtz'] ) )
    try:
        t_flag_F, t_flag_SIGF, t_flag_FREE = ample_util.get_mtz_flags( amopt.d['mtz'] )
    except Exception,e:
        logger.critical("Error generating flags from MTZ file: {0}\n{1}\n\nDo you need to run the CCP4 uniqueify program on the MTZ file?".format(amopt.d['mtz'],e) )
        sys.exit(1)
    
if not amopt.d['F']:
    amopt.d['F'] = t_flag_F
    
if not amopt.d['SIGF']:
    amopt.d['SIGF'] = t_flag_SIGF
    
if not amopt.d['FREE']:
    amopt.d['FREE'] = t_flag_FREE

# Print out what is being done
if amopt.d['use_buccaneer']:
    logger.info('Rebuilding in Bucaneer')
else:
    logger.info('Not rebuilding in Bucaneer')

if amopt.d['use_arpwarp']:
    logger.info('Rebuilding in ARP/wARP')
else:
    logger.info('Not rebuilding in ARP/wARP')

if amopt.d['make_frags']:
    logger.info('Making non homologusFragments')
else:
    logger.info('NOT Making Fragments')

if amopt.d['make_models']:
    logger.info('\nMaking Rosetta Models')
else:
    logger.info('NOT Making Rosetta Models')

logger.info('All needed programs are found, continuing Run')

#----------------------------
# Running is the 'official' output file 
#----------------------------
RUNNING = open( amopt.d['work_dir'] + '/ROSETTA.log', "w")
RUNNING.write( ample_util.header )
RUNNING.flush()

#----------------------------
# params used
#---------------------------
f = open( os.path.join( amopt.d['work_dir'], 'params_used.txt' ), "w") 
param_str = amopt.prettify_parameters()
f.write( param_str )
f.close()
# Echo to log too
logger.debug( param_str )

#######################################################
#
# SCRIPT PROPER STARTS HERE
#
######################################################
time_start = time.time()

#-----------------------------------
# Do The Modelling
#-----------------------------------

#-----------------------------------
# Make Rosetta fragments
#-----------------------------------
if amopt.d['make_frags']:
    rosetta_modeller.generate_fragments()

#-----------------------------------
# break here for NMR (frags needed but not modelling
# if NMR process models first
#-----------------------------------
if NMR_PROTOCOL:
    nmr.doNMR(amopt, logger)
# return from nmr with models already made

#-----------------------------------
# Make the models
#-----------------------------------
if amopt.d['make_models']:
    
    logger.info('----- making Rosetta models--------')
    logger.info('making ' + str(amopt.d['nmodels']) + ' models...')
    
    # If we are running with cluster support submit all modelling jobs to the cluster queue
    if amopt.d['submit_cluster']:
        
        # Generate the list of random seeds
        rosetta_modeller.generate_seeds( amopt.d['nmodels'] )

        # Invoke the cluster run class
        cluster_run = clusterize.ClusterRun()
        cluster_run.QTYPE = amopt.d['submit_qtype']
        cluster_run.setModeller( rosetta_modeller )
        cluster_run.setupModellingDir( amopt.d['work_dir'] )
        
        logger.info('Submitting {0} jobs to a queueing system of type: {1}\n'.format( amopt.d['nmodels'],  amopt.d['submit_qtype'] ) )
        # loop over the number of models and submit a job to the cluster
        for i in range( amopt.d['nmodels'] ):
            nproc=1 # each modelling job runs on a single processor
            jobNumber=i+1
            cluster_run.modelOnCluster( nproc, jobNumber )

        # Monitor the cluster queue to see when all jobs have finished
        cluster_run.monitorQueue()

    else:
        # Run locally
        amopt.d['models_dir'] = rosetta_modeller.doModelling()
        
    ##End IF amopt.d['submit_cluster']
    
    msg = '\nModelling complete - models stored in:\n   ' + amopt.d['models_dir'] + '\n\n'
elif amopt.d['import_models']:
    msg = '\nImporting models from directory:\n   ' + amopt.d['models_dir'] + '\n\n'
elif amopt.d['import_ensembles']:
    msg = '\nImporting ensembles from directory:\n   ' + amopt.d['ensembles_dir'] + '\n\n'

    
RUNNING.write(msg)
logger.info(msg)
    

#--------------------------------------------
# check if models are present regardless
#--------------------------------------------
if amopt.d['import_cluster']:
    raise RuntimeError,"import_cluster needs to be updated!"
    if os.path.exists(amopt.d['import_cluster']): 
        cluster_path = amopt.d['import_cluster']
        fcluster_dir = os.path.join( amopt.d['work_dir'], 'fine_cluster_1' )
        if not os.path.exists( fcluster_dir  ):
            os.mkdir( fcluster_dir )
        os.chdir( fcluster_dir )
        list_of_ensembles = truncateedit_MAX.truncate( amopt.d['theseus_exe'],
                                                       cluster_path ,
                                                       fcluster_dir ,
                                                       amopt.d['maxcluster_exe'], 
                                                       amopt.d['percent'] )

        os.system('mkdir ' + amopt.d['work_dir'] + '/ensembles_1')
        #for each_ens in list_of_ensembles:
        #    pdb_edit.edit_sidechains(each_ens, amopt.d['work_dir'] + '/ensembles_1/')
        #sys.exit()

#---------------------------------------
# Do the clustering
#---------------------------------------
ensembles = [] # List of ensembles - 1 per cluster

## Save results
#f = open( amopt.d['results_path'], 'w' )
#cPickle.dump( amopt.d, f )
#f.close()
#logging.info("Saved results as file: {0}\n".format( amopt.d['results_path'] ) )
#

if amopt.d['import_ensembles']:
    #---------------------------------------
    # Importing pre-made ensembles
    #---------------------------------------
    # Set list of ensembles to the one we are importing
    logger.info("Importing existing ensembles from: {0}".format( amopt.d['ensembles_dir'] ) )
    final_ensembles = []
    for infile in glob.glob(os.path.join(amopt.d['ensembles_dir'], '*.pdb')):
        final_ensembles.append(infile)

    if amopt.d['top_model_only']:
        logger.info("Only using the top model" )
        final_ensembles = truncateedit_MAX.One_model_only(final_ensembles , amopt.d['work_dir'])
        
    # Set ensembles
    ensembles = [ final_ensembles ]

else:
    
    if amopt.d['submit_cluster']:

        # Pickle dictionary so it can be opened by the job to get the parameters
        f = open( amopt.d['results_path'], 'w' )
        cPickle.dump( amopt.d, f )
        f.close()

        mrBuild = clusterize.ClusterRun()
        mrBuild.QTYPE = amopt.d['submit_qtype']
        mrBuild.ensembleOnCluster( amopt.d )
        mrBuild.monitorQueue()
            
        # queue finished so unpickle results
        f = open( amopt.d['results_path'], "r" )
        amopt.d = cPickle.load( f )
        f.close()
    else:
        ensemble.create_ensembles( amopt.d )

    # Check we have something to work with
    if not amopt.d.has_key('ensemble_results') or not len( amopt.d['ensemble_results'] ):
        msg = "Could not load any ensembles after running create_ensembles!"
        logger.critical( msg )
        sys.exit(1)
    
    # Extract list of pdbs from ensemble results
    for eresult in amopt.d['ensemble_results']:
        ensemble_pdbs = []
        for result in eresult:
            ensemble_pdbs.append( result.pdb )
        ensembles.append( ensemble_pdbs )

ensemble_summary = ensemble.ensemble_summary(amopt.d)
RUNNING.write(ensemble_summary)
RUNNING.flush()
logger.info(ensemble_summary)

#---------------------------------------
# MR BUMP analysis of the ensembles
#---------------------------------------
bump_dir = os.path.join(amopt.d['work_dir'], 'MRBUMP')
logger.info("Running MRBUMP jobs in directory: {0}".format( bump_dir ) )
if not os.path.exists(bump_dir):
    os.mkdir(bump_dir)
os.chdir(bump_dir)
amopt.d['mrbump_dir'] = bump_dir
amopt.d['mrbump_results'] = []

# Loop over clusters
for i, ensemble_pdbs in enumerate(ensembles):
    
    clusterID=str(i+1)
    logger.info('----- Running MRBUMP (cluster ' + clusterID+ ')--------\n\n')
    logger.info("Running {0} MRBUMP jobs for cluster: {1}".format( len(ensemble_pdbs),  clusterID ) )
    
    if len(ensemble_pdbs) < 1:
        msg = "ERROR! Cannot run MRBUMP for cluster {0} as there are no ensembles!".format( clusterID )
        logger.critical( msg )
        continue
    
    job_dir = os.path.join( amopt.d['mrbump_dir'], 'cluster_'+clusterID )

    logger.info("Running cluster {0} in directory: {1}".format( clusterID, job_dir ) )
    if not os.path.exists( job_dir ):
        os.mkdir( job_dir )
    os.chdir( job_dir )
    
    # Create job scripts
    logger.info("Generating MRBUMP runscripts in: {0}".format( job_dir ) )
    job_scripts = mrbump_ensemble.generate_jobscripts( ensemble_pdbs, amopt.d )
    
    if amopt.d['submit_cluster']:
        mrbump_ensemble.mrbump_ensemble_cluster( job_scripts, amopt.d )
    else:
        mrbump_ensemble.mrbump_ensemble_local( job_scripts, amopt.d )
        
    # Collect the MRBUMP results
    results_summary = mrbump_results.ResultsSummary( job_dir )
    results_summary.extractResults()
    amopt.d['mrbump_results'].append( results_summary.results )
    #summary = results_summary.summariseResults()
    
    
# Timing data
time_stop = time.time()
elapsed_time = time_stop - time_start
run_in_min = elapsed_time / 60
run_in_hours = run_in_min / 60
msg = '\nMR and shelx DONE\n\n ALL DONE  (in ' + str(run_in_hours) + ' hours) \n----------------------------------------\n'
logging.info(msg)
RUNNING.write(msg)
RUNNING.flush()

# Save results
f = open( amopt.d['results_path'], 'w' )
cPickle.dump( amopt.d, f )
f.close()
logging.info("Saved results as file: {0}\n".format( amopt.d['results_path'] ) )

# Now print out the final summary
summary = amopt.final_summary()
logger.info( summary )
RUNNING.write(summary)
RUNNING.flush()
RUNNING.close()

# Copy best pdb to output_pdb - hack - just take best from the first cluster#
output_pdb = os.path.join( amopt.d['work_dir'], amopt.d['output_pdb'])
logging.info("Copying result pdb to: {0}\n".format( output_pdb ) )
shutil.copy2( amopt.d['mrbump_results'][0][0].pdb, output_pdb )

# os.system('tar -cvf '+pdb_code+'_' + work_dir+'.tar '+work_dir)
# os.system('gzip ' +pdb_code+'_'+ work_dir+'.tar')
# os.system('mv '+ pdb_code+'_'+  work_dir+'.tar.gz /data2/jac45 ')
# os.system('rm '+work_dir)
sys.exit(0)
#------------------------------------
# END
#----------------------------------
